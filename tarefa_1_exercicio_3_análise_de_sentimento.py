# -*- coding: utf-8 -*-
"""TAREFA_1_Exercicio_3_Análise_de_Sentimento.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yFPB4sefpIdm7UAeZWBGioI5UMbdlVSW
"""

!pip install unidecode

# Montando o drive onde estão armazenados os dados
from google.colab import drive
drive.mount('/content/drive')

"""### Imports"""

import pandas as pd
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import TextClassificationPipeline
from transformers import pipeline
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import spacy

import nltk
nltk.download('stopwords')
nltk.download('rslp')
nltk.download('punkt')

from nltk.stem import RSLPStemmer

from unidecode import unidecode

"""### Load DataFrame"""

# Load the CSV file into a DataFrame
file_path = '/content/drive/MyDrive/ANATEL_SELECAO/tb_google_play_comentarios.csv'
df = pd.read_csv(file_path)

df

df = df[['content','score']]

"""### Funções de limpeza"""

# nlp = spacy.load('pt_core_news_lg')
stop_words_set = set(stopwords.words('portuguese'))
unidec_stop_words = set()
for stop_word in stop_words_set:
    unidec_stop_words.add(unidecode(stop_word))

stop_words = unidec_stop_words - set(['muito', 'nao'])
# stemmer = RSLPStemmer()

def clean_text(text):

    text = text.lower()

    # Filtra caracteres especiais
    text = re.sub(r'[^a-zA-Z\sáÁéÉíÍóÓúÚâ êÊîÎôÔûÛãÃõÕçÇ]', ' ', text)

    tokens = word_tokenize(text, language='portuguese')
    tokens = [word for word in tokens if unidecode(word) not in stop_words] # Remove stopwords

    # Remove espaços em branco extras
    text = ' '.join(tokens)

    text = text.lower()

    text = unidecode(text)

    return text

df["clean_content"] = df.content.apply(lambda x: clean_text(x))

"""### BERT"""

df.head()

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name)

# Criar um pipeline de classificação de texto
pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer, framework='pt')

# Função para classificar o sentimento
def classify_sentiment(text):
    try:
        # Prever o sentimento
        result = pipeline(text)
        # Extrair o rótulo com a maior pontuação
        return result[0]['label']
    except Exception as e:
        print(f"Erro: {e}")
        return None

# Aplicar a função de classificação de sentimento à coluna 'content'
df['sentiment'] = df['content'].apply(classify_sentiment)

def classify_sentiment(sentiment):
    if sentiment in ["1 star", "2 stars"]:
        return "negative"
    elif sentiment == "3 stars":
        return "neutral"
    elif sentiment in ["4 stars", "5 stars"]:
        return "positive"
    else:
        return None  # Para lidar com valores inesperados

# Aplicar a função de classificação à coluna 'sentiment' para criar a coluna 'label'
df['label'] = df['sentiment'].apply(classify_sentiment)

df.head()

df['label'].value_counts()