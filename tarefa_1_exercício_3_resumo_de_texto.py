# -*- coding: utf-8 -*-
"""TAREFA_1_Exercício_3_Resumo_de_Texto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wiK3GJJd_1x2NME9tIdgBTV3ojLAwmdx
"""

!pip install datasets

# Montando o drive onde estão armazenados os dados
from google.colab import drive
drive.mount('/content/drive')

# Importar bibliotecas necessárias
from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments
from transformers import T5Tokenizer, T5ForConditionalGeneration
from datasets import load_dataset, load_metric

# 1. Carregar o modelo pré-treinado e o tokenizer
#model_name = "facebook/bart-large-cnn"  # ou "t5-small", "t5-base", etc.
#tokenizer = BartTokenizer.from_pretrained(model_name)
tokenizer = T5Tokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")

# Carregar uma parte da base de dados
dataset = load_dataset("cnn_dailymail", "3.0.0", split={"train": "train[:1%]", "validation": "validation[:1%]"})

# Verificar o tamanho dos dados carregados
print(f"Training data size: {len(dataset['train'])}")
print(f"Validation data size: {len(dataset['validation'])}")

def preprocess_function(examples):
    inputs = examples["article"]
    targets = examples["highlights"]

    # Tokenizar o texto de entrada
    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding='max_length')

    # Tokenizar o resumo
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=150, truncation=True, padding='max_length')

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Aplicar a função de preprocessamento
tokenized_datasets = dataset.map(preprocess_function, batched=True)

# 3. Definir parâmetros de treinamento
training_args = TrainingArguments(
    output_dir="./results",           # Diretório onde os resultados serão salvos
    evaluation_strategy="epoch",      # Avaliar a cada época
    learning_rate=2e-5,               # Taxa de aprendizado
    per_device_train_batch_size=4,   # Tamanho do batch de treinamento
    per_device_eval_batch_size=4,    # Tamanho do batch de avaliação
    num_train_epochs=3,              # Número de épocas
    weight_decay=0.01,               # Decaimento do peso
    logging_dir="./logs",            # Diretório para logs
    logging_steps=10,
)

from transformers import DataCollatorForSeq2Seq

# 4. Definir o Trainer com o data collator personalizado

# Criar um data collator para tarefas de seq2seq
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# Definir o Trainer com o data collator personalizado
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator
)

# 5. Treinar o modelo
trainer.train()

# 6. Avaliar o modelo
eval_results = trainer.evaluate()

print("Avaliação:", eval_results)

# 7. Salvar o modelo fine-tuned
model.save_pretrained("/content/drive/MyDrive/ANATEL_SELECAO/model")
tokenizer.save_pretrained("/content/drive/MyDrive/ANATEL_SELECAO/model")

"""# **Exemplo de uso**"""

from transformers import BartTokenizer, BartForConditionalGeneration

# 1. Carregar o modelo e o tokenizer fine-tuned
model_path = "/content/drive/MyDrive/ANATEL_SELECAO/model"  # Substitua pelo caminho onde você salvou o modelo fine-tuned
tokenizer = BartTokenizer.from_pretrained(model_path)
model = BartForConditionalGeneration.from_pretrained(model_path)

# 2. Função para gerar resumos
def generate_summary(text, max_length=150, min_length=50, length_penalty=2.0):
    inputs = tokenizer.encode("summarize: " + text, return_tensors="pt", max_length=1024, truncation=True)
    summary_ids = model.generate(
        inputs,
        max_length=max_length,
        min_length=min_length,
        length_penalty=length_penalty,
        num_beams=4,
        early_stopping=True
    )
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# 3. Exemplo de texto a ser resumido
text = """
A inteligência artificial (IA) é um campo da ciência da computação que se preocupa com a criação de sistemas capazes de realizar tarefas que normalmente exigem inteligência humana. Estes sistemas podem aprender e se adaptar a partir de novos dados, melhorar seu desempenho e fazer previsões baseadas em informações passadas. Exemplos de aplicações de IA incluem reconhecimento de fala, visão computacional e processamento de linguagem natural. A IA tem o potencial de transformar muitos aspectos da vida moderna, desde a automação industrial até a personalização de serviços e produtos.
"""

# 4. Gerar o resumo
summary = generate_summary(text)
print("Resumo Gerado:")
print(summary)